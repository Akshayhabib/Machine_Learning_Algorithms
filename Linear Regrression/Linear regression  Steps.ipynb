{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b7f45d",
   "metadata": {},
   "source": [
    "# Linear regression steps :\n",
    "- 1. Create the dataframe properly -- > pd.read_csv ( ), pd.read_excel ( )\n",
    "- 2. processing the data : \n",
    "                    a . Feature selection -- > domain knowledge -- > drop ( )\n",
    "                    b . Handling missing values -- > isnull( ).sum( ) , fillna( ) , dropna( )\n",
    "- 3. Assumption 1 : There should be no outliers in the data -- > boxplot ( ) , drop ( )\n",
    "- 4. Assumption 2 : Assumption of Linearity : Every ind var should have a linear relationship with the dep var -- > pairplot()\n",
    "- 5. Create X and Y -- > X = ind vars , Y = dep var\n",
    "- 6. Assumption 3 : Assumption of Normality : The dependent variable should follow an approximate normal distribtion --> distplot(),log()\n",
    "- 7. Check and handle the skewness in the ind vars -- > hist ( ) , skew ( ) , log1p ( )\n",
    "- 8. Assumption 4 : Assumption of no multicollinearity : There should be no multicollinearity between the independent variables -->corr(),heatmap(),vif()\n",
    "\n",
    "- 9. Splitting the data into train and test ( validation_test ) -- > train_test_split ( )\n",
    "- 10. Building the model :\n",
    "               a . Create the model object -- > obj = AlgoName ( )\n",
    "               b . Train the model -- > obj.fit ( X_train , Y_train )\n",
    "               c . Predict using the model -- > Y_pred = obj.predict ( x_test )\n",
    "- 11. Evaluate the model : square , Adjusted R - square , RMSE , AIC / BIC , p - values\n",
    "- 12. Assumption 5 : There should be no autocorrelation in the data -- > Durbin - Watson Test\n",
    "- 13. Assumption 6 : Errors should be random -- > Fitted vs Residual plot\n",
    "- 14. Assumption 7 : Errors should follow an approximate normal distribution -- > Normal QQ plot\n",
    "- 15. Assumption 8 : Errors should follow a constant variance ( Homoskedasticity ) -- Scale Location plot\n",
    "- 16. Tuning the model :\n",
    "             a . Feature selection\n",
    "             b . Regularization techniques -- > Ridge ( ) , Lasso ( )\n",
    "             c . Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a379cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
